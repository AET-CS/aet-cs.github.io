{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d783f5-82da-4038-ad8d-0511640d0329",
   "metadata": {},
   "source": [
    "# Cancer Data Logistic Regression (Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cffd6946-2639-4aec-968a-bf3d93710881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ccfe0-7b76-4d24-9677-cc617430f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from scipy.stats import chi2_contingency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253a80b-53d5-4af4-98b8-660f5e6a9b0e",
   "metadata": {},
   "source": [
    "Uncomment if you need the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338ce81-67f1-4f34-b5f8-49cdd60cae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://aet-cs.github.io/white/ML/data/Cancer_Data.csv\n",
    "!wget https://aet-cs.github.io/white/ML/data/Cancer_Data_Cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974a2c0-3ecb-47a7-9171-5946e32e243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../data\" ## change this line if needed\n",
    "filename = \"Cancer_Data.csv\"\n",
    "filepath = os.path.join(data_root, filename)\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980e7b7-64d9-4d2a-9023-af2d40ea6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.drop(['radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcef77-ddd8-4757-b388-d03b4a09a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df47d5-ec4d-4fe2-aa96-a27827b9b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabc98d-57b1-4de6-af0a-722c5f02b998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a69e8-01b3-4bb6-aace-a78c0c81af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(df, one_hot_encode = False):\n",
    "    # Use sklearn Imputers to fill in the categorical and numerical columns\n",
    "    simple_median = SimpleImputer(strategy='median')\n",
    "    simple_most_freq = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    num_cols = df.select_dtypes(include=np.number).columns # numerical data\n",
    "    cat_cols = df.select_dtypes(include=object).columns # categorical data\n",
    "\n",
    "    # Handle missing values\n",
    "    df[num_cols] = simple_median.fit_transform(df[num_cols])\n",
    "    df[cat_cols] = simple_most_freq.fit_transform(df[cat_cols])\n",
    "\n",
    "    # Replace infinite values with NaN and then impute\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df[num_cols] = simple_median.fit_transform(df[num_cols])\n",
    "\n",
    "    if one_hot_encode:\n",
    "        O_encoder = OrdinalEncoder()\n",
    "        df[cat_cols]= O_encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47867d-159d-4c30-8bd2-13646c2b83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train(df, test_size = 0.2, random_state = True):\n",
    "    target = \"diagnosis\"\n",
    "    X = df.drop(target, axis=1)\n",
    "\n",
    "    # Robust scaling to handle outliers, based on median and inter-quartile range\n",
    "    scaler = preprocessing.RobustScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    # Additional check for any remaining infinite values\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    y = df[target]\n",
    "    if random_state is True:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c1c2a-a4d4-46af-8a92-3d0043a549ef",
   "metadata": {},
   "source": [
    "## Start Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4b5f4-62c6-4e00-a1ad-229c91c98777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines would load the data locally\n",
    "data_root = \"../data\"\n",
    "filename = \"cancer_data_cleaned.csv\"\n",
    "filepath = os.path.join(data_root, filename)\n",
    "\n",
    "# Perform a logistic regression\n",
    "df = get_data(filepath)\n",
    "df = pre_process_data(df, one_hot_encode = True)\n",
    "X_train, X_test, y_train, y_test = get_test_train(df, random_state = False)\n",
    "\n",
    "# Create logistic regression with liblinear solver\n",
    "lreg = LogisticRegression(\n",
    "    C=1.0,\n",
    "    solver='liblinear',  # Different solver that might be more stable\n",
    "    max_iter=10000,  # increased from default because of squirrely data\n",
    "    tol=1e-4, # tolerance set to 0.0001\n",
    ")\n",
    "model = lreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1aee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f83cd-8046-4280-83c5-fa0cbc232b87",
   "metadata": {},
   "source": [
    "Get the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730f9d7-6805-4f7d-b2e8-0fe4b08ed0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lreg.predict(X_test[0:1])\n",
    "print(\"Prediction for first sample:\", pred[0])\n",
    "print(f\"Train accuracy  = {model.score(X_train, y_train):.3}\")\n",
    "print(f\"Test  accuracy  = {model.score(X_test, y_test):.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e12c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from the original dataframe (excluding the target column)\n",
    "feature_names = df.drop('diagnosis', axis=1).columns\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lreg.coef_[0],\n",
    "    'Abs_Coefficient': abs(lreg.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"Model Coefficients (sorted by absolute value):\")\n",
    "print(coef_df)\n",
    "\n",
    "print(\"\\nIntercept:\", lreg.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a42e13-5479-40af-ac2f-d20ada740acf",
   "metadata": {},
   "source": [
    "Quick snapshot of the confusion matrix (rows are truth  0/1 and cols are predictions 0/1). You can shift-tab on the parens the see the method signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e090a27-f2ee-4bff-9e8b-585b596a6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_pred = model.predict(X_test), y_true = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e10ab-5201-4902-94d8-cce010c2ff1a",
   "metadata": {},
   "source": [
    "We want to get the probabilites from X-test, NOT the classifications. So we want real values in (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3d0ef-ed32-4760-ab88-2c08ad084104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the test data\n",
    "y_prob = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78110b-0b92-4f5f-8612-20492189843c",
   "metadata": {},
   "source": [
    "If we sort y_prob and y_test in the same order, then we can make a reasonable plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd248e-3fd1-4444-bf99-95f5854afb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data for plotting\n",
    "y_pred = model.predict(X_test)\n",
    "sorted_indices = np.argsort(y_prob)  ## argsort returns the indices ordered by the key values, so we can copy the sort order around\n",
    "sorted_y_prob = y_prob[sorted_indices]\n",
    "sorted_y_test = np.array(y_test)[sorted_indices]\n",
    "sorted_y_pred = np.array(y_pred)[np.argsort(y_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f68329-3661-4ddf-9116-3dbdbc6df4cc",
   "metadata": {},
   "source": [
    "Print the vector of sorted_y_prob and of sorted_y_test to verify the are generally increasing from 0 to 1. We create a dataframe also just because."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06778ca8-1b1e-469d-8589-96a68d373cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "results_df = pd.DataFrame({\"Probs\": sorted_y_prob, \n",
    "                           \"True y\": sorted_y_test,\n",
    "                           \"Pred y\":  sorted_y_pred})\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7de2d8-e1a6-485f-88a2-2082a59afebe",
   "metadata": {},
   "source": [
    "Now we can plot the logistic model, along with the true and predicted values. We set the split point at 0.5, but it can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7619a-bd23-45c6-b40d-47195836f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# find the index of the first value in y_prob that is greater than split_point\n",
    "split_point = 0.5\n",
    "index = np.searchsorted(sorted_y_prob, split_point)\n",
    "new_y_pred = np.where(sorted_y_prob > split_point, 1, 0)\n",
    "\n",
    "x_test = np.array(range(len(y_test))) ## just a counter \n",
    "# For demonstration if needed:\n",
    "# x_test = np.linspace(-3, 3, len(y_test))\n",
    "\n",
    "# Compute confusion categories\n",
    "# these numpy calculation return a LIST of INDICES, which is very powerful!\n",
    "# see how they're used below\n",
    "tp = (sorted_y_test == 1) & (new_y_pred == 1)\n",
    "tn = (sorted_y_test == 0) & (new_y_pred == 0)\n",
    "fp = (sorted_y_test == 0) & (new_y_pred == 1)\n",
    "fn = (sorted_y_test == 1) & (new_y_pred == 0)\n",
    "\n",
    "# --- Plot setup ---\n",
    "plt.figure(figsize=(12 ,5))\n",
    "\n",
    "# Logistic curve (predicted probabilities)\n",
    "plt.plot(x_test, sorted_y_prob, color='black', linewidth=2, label='Predicted probability')\n",
    "\n",
    "# Scatter for each category\n",
    "plt.scatter(x_test[tp], sorted_y_test[tp], color='blue', s=40, label='True Positive')\n",
    "plt.scatter(x_test[tn], sorted_y_test[tn], color='green', s=40, label='True Negative')\n",
    "plt.scatter(x_test[fp], sorted_y_test[fp], color='red', s=40, label='False Positive')\n",
    "plt.scatter(x_test[fn], sorted_y_test[fn], color='orange', s=40, label='False Negative')\n",
    "\n",
    "# Optional: plot your custom split point threshold line\n",
    "plt.axhline(split_point, color='gray', linestyle='--', label=f'Split point = {split_point:.2f}')\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('x index')\n",
    "plt.ylabel('Probability / True label')\n",
    "plt.title('Logistic Regression Classification Outcomes')\n",
    "plt.legend(loc='center left', frameon=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39764367-1a82-49b2-b94f-7e14434e54b2",
   "metadata": {},
   "source": [
    "# Binary Classification Helper Functions\n",
    "\n",
    "Binary classification for logistic regression relies on knowing where the **split point** (threshold) is.\n",
    "By default, this split point is `alpha = 0.5`, meaning that predicted probabilities greater than 0.5\n",
    "are classified as `1` (positive), and those less than or equal to 0.5 are classified as `0` (negative).\n",
    "\n",
    "However, `alpha = 0.5` is not always optimal ‚Äî depending on the dataset and the cost of errors, \n",
    "a different threshold may produce better results.\n",
    "\n",
    "In this exercise, you will define a set of helper functions to analyze binary classification results \n",
    "and help determine the optimal value of `alpha`.  \n",
    "\n",
    "Each function will help you compute parts of the **confusion matrix** and related performance metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úèÔ∏è Your tasks\n",
    "\n",
    "1. **binary_classify(y_prob, alpha)**  \n",
    "   - Given a vector of predicted probabilities (`y_prob`), return a vector of 0s and 1s  \n",
    "     depending on whether each probability exceeds `alpha`.\n",
    "\n",
    "2. **tp(true, observed)**  \n",
    "   - Return the number of *true positives*: cases where both the true label and the prediction are 1.\n",
    "\n",
    "3. **tn(true, observed)**  \n",
    "   - Return the number of *true negatives*: cases where both are 0.\n",
    "\n",
    "4. **fp(true, observed)**  \n",
    "   - Return the number of *false positives*: predicted 1 but actually 0.\n",
    "\n",
    "5. **fn(true, observed)**  \n",
    "   - Return the number of *false negatives*: predicted 0 but actually 1.\n",
    "\n",
    "6. **precision(true, observed)**  \n",
    "   - Compute precision = TP / (TP + FP).  \n",
    "     (Handle divide-by-zero cases gracefully.)\n",
    "\n",
    "7. **recall(true, observed)**  \n",
    "   - Compute recall = TP / (TP + FN).  \n",
    "     (Handle divide-by-zero cases gracefully.)\n",
    "\n",
    "8. **f1(true, observed, weights)**  \n",
    "   - Return a *f1* using  (TP, FP, TN, FN).  \n",
    "\n",
    "---\n",
    "\n",
    "Once you‚Äôve implemented these, you‚Äôll be able to:\n",
    "- Explore how changing `alpha` affects precision, recall, and F1 score,  \n",
    "- Visualize the trade-off between sensitivity and specificity, and  \n",
    "- Find the threshold that maximizes your chosen performance metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f6a74-7df7-47e8-b105-23834afa35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def binary_classify(y_prob, alpha):\n",
    "    \"\"\"Return 1 if probability > alpha, else 0.\"\"\"\n",
    "\n",
    "def tp(true, observed):\n",
    "    \"\"\"True positives: predicted 1, actually 1.\"\"\"\n",
    "\n",
    "\n",
    "def tn(true, observed):\n",
    "    \"\"\"True negatives: predicted 0, actually 0.\"\"\"\n",
    "\n",
    "\n",
    "def fp(true, observed):\n",
    "    \"\"\"False positives: predicted 1, actually 0.\"\"\"\n",
    "\n",
    "\n",
    "def fn(true, observed):\n",
    "    \"\"\"False negatives: predicted 0, actually 1.\"\"\"\n",
    "\n",
    "\n",
    "def precision(true, observed):\n",
    "    \"\"\"Precision = TP / (TP + FP)\"\"\"\n",
    "\n",
    "def recall(true, observed):\n",
    "    \"\"\"Recall = TP / (TP + FN)\"\"\"\n",
    "\n",
    "def f1(true, observed):\n",
    "    \"\"\"F1 = 2 * (precision * recall) / (precision + recall)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644dcc25-4f8e-44a2-ab23-4ca54b34be7f",
   "metadata": {},
   "source": [
    "## Step 1: Test your classification and precision functions\n",
    "\n",
    "Before experimenting with different thresholds (`alpha`), let's verify that your helper \n",
    "functions are working correctly.\n",
    "\n",
    "We'll start by classifying predictions using the default split point `alpha = 0.5`, \n",
    "which is standard for logistic regression. Then we'll measure **precision**, which tells us:\n",
    "\n",
    "> Of all the points predicted as positive (1), how many were actually positive?\n",
    "\n",
    "A high precision means your model makes few false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d29f6-8df4-4220-9223-524f94123d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = binary_classify(sorted_y_prob, 0.5)\n",
    "q = zip(observed, sorted_y_test)\n",
    "print(precision(sorted_y_test, observed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74922cce-e25a-4eb3-981e-39408d41135e",
   "metadata": {},
   "source": [
    "## Step 2: Precision‚ÄìRecall Trade-off\n",
    "\n",
    "In binary classification, changing the threshold `Œ±` (alpha) controls how ‚Äústrict‚Äù your model is \n",
    "when deciding what counts as a positive prediction.\n",
    "\n",
    "- A **low Œ±** means you label more cases as positive ‚Üí higher recall, lower precision.\n",
    "- A **high Œ±** means you label fewer cases as positive ‚Üí lower recall, higher precision.\n",
    "\n",
    "In this exercise, you‚Äôll explore this trade-off by varying `Œ±` from 0 to 1 (in steps of 0.01) and plotting\n",
    "the resulting **precision** and **recall** values.\n",
    "\n",
    "Your goal: complete the loop below to compute `precision(Œ±)` and `recall(Œ±)` for each threshold,\n",
    "and then visualize them on a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de878a-5a65-483b-ba56-92e4c970219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [] #empty list of precisions\n",
    "rs = [] #empty list of recalls\n",
    "for i in range(100):\n",
    "    alpha = i/100.0\n",
    "    # compute results\n",
    "    # and add values to the precisions and recalls vectors\n",
    "\n",
    "\n",
    "# --- Plot precision vs recall ---\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(ps, rs, marker='o', markersize=3, linestyle='-', color='royalblue')\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Precision‚ÄìRecall Curve for Varying Œ± (Threshold)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# --- Annotate alpha values at selected points ---\n",
    "for i in range(0, 100, 10):  # Label every 10th alpha value\n",
    "    alpha = i / 100.0\n",
    "    # Offset labels slightly for readability\n",
    "    plt.annotate(f'Œ±={alpha:.2f}', \n",
    "                 (ps[i], rs[i]),\n",
    "                 textcoords=\"offset points\", \n",
    "                 xytext=(8, -8), \n",
    "                 ha='left', fontsize=8,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", alpha=0.6, lw=0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f107b-1af8-4fc1-bfa9-5589ddc32a01",
   "metadata": {},
   "source": [
    "## Step 3: Reflect on what you see\n",
    "\n",
    "Look carefully at your Precision‚ÄìRecall plot. Think about how changing Œ± (the classification threshold)\n",
    "affects your model‚Äôs behavior.\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. What general pattern do you notice between **precision** and **recall** as Œ± increases?\n",
    "   - Why does this happen?\n",
    "\n",
    "2. Where on the plot do you see the ‚Äúbest balance‚Äù between precision and recall?\n",
    "   - How would you decide what balance is best for a particular application?\n",
    "\n",
    "3. Suppose your model is being used for **disease detection** or **fraud detection**:\n",
    "   - Would you prefer higher precision or higher recall? Why?\n",
    "\n",
    "4. Did Œ± = 0.5 (the default) seem like a good threshold for this dataset?\n",
    "   - If not, what would you change it to, and why?\n",
    "\n",
    "5. What happens near the extreme Œ± values (very small or very large)?\n",
    "   - How does that relate to how often the model predicts ‚Äúpositive‚Äù?\n",
    "\n",
    "Take a moment to discuss or write down your thoughts before moving on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092144c-f298-4324-a56c-6dc6667b5422",
   "metadata": {},
   "source": [
    "Now we want to see how weighted F1-scores vary as a function of alpha. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8d710-ce93-4d40-bd4e-35b738b5e1ba",
   "metadata": {},
   "source": [
    "### üß† Reflection and Visualization\n",
    "\n",
    "Before we try to find the best threshold automatically, let‚Äôs *explore* what‚Äôs happening.\n",
    "\n",
    "üëâ **Reflection questions**\n",
    "\n",
    "1. What do you think happens to **precision** as we raise the threshold?\n",
    "2. What about **recall**?\n",
    "3. Where do you think the **F1 score** might peak ‚Äî closer to low thresholds or high ones?\n",
    "\n",
    "Now, let‚Äôs **plot all three metrics** versus the classification threshold to see what‚Äôs going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df5a6b-d808-469a-bd14-aab9ec008709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds to test\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "# Calculate precision, recall, F1 for each threshold\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    ### make 3 vectors to plot below\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(thresholds, precisions, label=\"Precision\")\n",
    "plt.plot(thresholds, recalls, label=\"Recall\")\n",
    "plt.plot(thresholds, f1_scores, label=\"F1 Score\", linewidth=2)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall, and F1 vs. Classification Threshold\")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba67952-374f-4345-9282-1721252f8ba7",
   "metadata": {},
   "source": [
    "## Step 4: Finding the optimal split-point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1ab48-225a-4e07-9d0c-9013d589d374",
   "metadata": {},
   "source": [
    "‚úÖ **Follow-up (next cell)**\n",
    "In the next step, we‚Äôll use `numpy` to *find the threshold* that gives the **maximum F1 score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc175b2-44ba-4387-aacc-612c8001f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca916baf-de16-43d4-84b8-0aeff15028cc",
   "metadata": {},
   "source": [
    "‚úÖ **The best alpha**\n",
    "Based on the previous result, determine the best alpha and fill it in below for opt_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665399e3-555f-4333-bbdf-d9698518a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha = ###\n",
    "\n",
    "observed = binary_classify(y_prob, opt_alpha)\n",
    "ps = precision(y_test, observed)\n",
    "rs = recall(y_test, observed)\n",
    "opt_tp = tp(y_test, observed)\n",
    "opt_tn = tn(y_test, observed)\n",
    "opt_fp = fp(y_test, observed)\n",
    "opt_fn = fn(y_test, observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34ead1-35f2-41c2-9ef6-2a9fff695b44",
   "metadata": {},
   "source": [
    "Now draw a nice confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aca928-f903-419d-9c8e-541b7658393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, observed, labels=lreg.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=lreg.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
