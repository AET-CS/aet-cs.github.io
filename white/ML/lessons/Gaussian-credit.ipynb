{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9602b13261afa9897bb74827af2c072d0fc6e87",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Credit Card Fraud Detection Using Multivariate Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Source: https://www.kaggle.com/code/hrao768/gaussian-distrib-for-anomaly-detection-f1-83/notebook\n",
    "* Accessed: Jan 15, 2025\n",
    "* Modified as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c9d4938-bc9a-420c-946e-04a2495b8f83",
    "_uuid": "6eed66c0e9a117cfcfa10736152c0009b54ff9a0"
   },
   "source": [
    "For training and evaluating Gaussian distribution algorithms, we are going to split the train, cross validation and test data sets using below ratios.\n",
    "\n",
    "    1) Train:  60% of the Genuine records (y=0), no Fraud records(y=1). So the training set will not have a label as well.\n",
    "    \n",
    "    2) CV:  20% of the Genuine records (y=0), 50% of the Fraud records(y=1)\n",
    "    \n",
    "    3) Test: Remaining 20% of the Genuine records(y=0), Remaining 50% of the Fraud records(y=1)\n",
    "\n",
    "\n",
    "\n",
    "Procedure for anomaly detection:\n",
    "\n",
    "    1) Fit the model p(x) on training set\n",
    "    \n",
    "    2) On cross validation/test data, predict\n",
    "    \n",
    "        y = 1 if p(x) < epsilon (anomaly)\n",
    "        \n",
    "        y = 0 if p(x) >= epsilon (normal)\n",
    "        \n",
    "    3) We use cross validation to choose parameter epsilon using the evaluation metrics Preceion/Recall, F1-score.\n",
    "    \n",
    "\n",
    "\n",
    "We could use couple of Gaussian distribution models for training anomaly detection.\n",
    "\n",
    "    1) Gaussian (Normal) Distribution - the normal distribution is parametrized in terms of the mean and the variance.\n",
    "    \n",
    "    2) Multivariate Normal Distribution - The probability density function for multivariate_normal is parametrized in terms of the mean and the covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f7a0b55-8aee-43d6-a1ba-341848ac1f1d",
    "_uuid": "e066626887c842c918422d3de1c37a298e956f96"
   },
   "source": [
    "Algorithm Selection:\n",
    "\n",
    "    1) For this dataset, we are going to use multivariate normal probability density function, since it automatically generates the relationships (correlation) between variables to calculate the probabilities. So we don't need to derive new features. As the features are outcome of PCA, it is difficult for us to understand the relationship between these features. \n",
    "\n",
    "    2) However multivariate normal probability density function is computationally expensive compared to normal Gaussian probability density function. On very large datasets, we might have to prefer Gaussian probability density function instead of multivariate normal probability density function to speed up the process and do feature engineering.\n",
    "\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "    1) Features that we choose for these algorithms have to be normally distributed. Otherwise we need to transform the features to normal distribution using log, sqrt etc.\n",
    "\n",
    "    2) Choose features that might take on unusually large or small values in the event of an anomaly. We looked at the distribution in the beginning using distplot. So it is wise to choose features which have completely different distribution for fraud records compared to genuine records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8910a9e-5a63-4b90-8f77-6ddaf72b3d8e",
    "_uuid": "0505c5fc2631957b110675fc7ed368068d945580"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict,cross_val_score, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix,classification_report,f1_score,recall_score,precision_score,accuracy_score,precision_recall_curve,roc_curve,roc_auc_score\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "849a3e0f-02ec-4d6f-adf1-2c3c08651a41",
    "_uuid": "649f475f74125199a224e7ddf1cfa069f37f2eaf"
   },
   "outputs": [],
   "source": [
    "def Print_Accuracy_Scores(y,y_pred):\n",
    "    print(\"F1 Score: \", f1_score(y,y_pred))\n",
    "    print(\"Precision Score: \", precision_score(y,y_pred))\n",
    "    print(\"Recall Score: \", recall_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b6081bbd-cfe9-4da7-aa69-85cff0b9ced6",
    "_uuid": "23cd5a58f6c4bb5762fda6275fb4278aace0ec25"
   },
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "# UNCOMMENT ONE\n",
    "\n",
    "url = \"https://github.com/AET-CS/aet-cs.github.io/blob/main/white/ML/data/creditcard.csv\"\n",
    "\n",
    "# cc_dataset = pd.read_csv(url=url)\n",
    "# cc_dataset = pd.read_csv(\"../data/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a6dff097-fed0-4eaa-9b6e-75089e2ebfaf",
    "_uuid": "81e24de0215641aea5e5a41cb00d299392570f05"
   },
   "outputs": [],
   "source": [
    "cc_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "341f9638-a2e2-4668-b75f-bc51aebbf98f",
    "_uuid": "da1cca1663135b3d8d0c54d76bcc663253e014b3"
   },
   "outputs": [],
   "source": [
    "cc_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c0d93bd-d1aa-4530-9976-1c77a075bb24",
    "_uuid": "bfc30b31be4fafb7fbe7c3530a0efdc0e41cb35d"
   },
   "outputs": [],
   "source": [
    "cc_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b440f4c7-b279-471b-9a6c-261a4472309b",
    "_uuid": "951456c8b760ad1a9058564563184e201e4394ed"
   },
   "outputs": [],
   "source": [
    "#Code for checking if any feature has null values. Here the output confirms that there are no null values in this data set.\n",
    "cc_dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e6a43fe-d32d-44b3-8c9d-1c94d3704835",
    "_uuid": "84bbe38151d228ee79f754874fcdc07cc245ed4d"
   },
   "outputs": [],
   "source": [
    "#Counts for each class in the dataset. As you can see, we have only 492 (0.17%) fraud cases out of 284807 records. Remaining 284315 (99.8%) of the records belong to genuine cases.\n",
    "#So the dataset is clearly imbalanced!\n",
    "cc_dataset['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "84bdcb0c-83a4-4e4a-b08f-5d42e603f6e1",
    "_uuid": "b588e7cd8aa7ca120a2f77307a8b700f203fd3b5"
   },
   "outputs": [],
   "source": [
    "#Data Visualization for checking the distribution for Genuine cases & Fraud cases for each feature\n",
    "v_features = cc_dataset.columns\n",
    "plt.figure(figsize=(12,31*4))\n",
    "gs = gridspec.GridSpec(31,1)\n",
    "\n",
    "for i, col in enumerate(v_features):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(cc_dataset[col][cc_dataset['Class']==0],color='g',label='Genuine Class')\n",
    "    sns.distplot(cc_dataset[col][cc_dataset['Class']==1],color='r',label='Fraud Class')\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "826e94d5-9c37-49cf-ae06-9e6ec516a5e5",
    "_uuid": "d434b5e3504beb48d04d634be29907b54f5c3aba"
   },
   "source": [
    "Feature selection: \n",
    "    1) We can see Normal Distribution of anomalous transactions (class = 1) is matching with Normal Distribution of genuine transactions (class = 0) for V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8' features. It is better to delete these features as they may not be useful in finding anomalous records.\n",
    "    2) Time is also not useful variable since it contains the seconds elapsed between the transaction for that record and the first transaction in the dataset. So the data is in increasing order always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "62960c8b-bac6-4e1f-b942-6a9e5f2b7019",
    "_uuid": "80fa9da12f8a3565e842ed748284db025abf08ad"
   },
   "outputs": [],
   "source": [
    "cc_dataset.drop(labels = ['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8','Time'], axis = 1, inplace=True)\n",
    "cc_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7b93ce61-35c0-444d-88b4-11a93a84300d",
    "_uuid": "ebd1f4596b6e29e679481ece6184c1fe9d524029"
   },
   "source": [
    "Below features doesn't have the same distribution for both genuine & fraud records. However distribution for fraud records is not unusual as well.\n",
    "So I'll delete these features as well, since the features with unusual behavior for Fraud records will be most useful in anomaly detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7981708a-547e-412a-b494-97a4d1cc130a",
    "_uuid": "9555dc9e72b84227e82c1c465c18feb98c7cd65a"
   },
   "outputs": [],
   "source": [
    "cc_dataset.drop(labels = ['V1','V2','V5','V6','V7','V21','Amount'], axis = 1, inplace=True)\n",
    "cc_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "003d5ebf-f0a0-41a0-944d-87ec4c4e68e3",
    "_uuid": "88af90a80052cc2c9d240570492442af97f3022a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualization to understand the relationship between features and also data pattern using pair plot from seaborn\n",
    "# cc_subset = cc_dataset.sample(frac=0.001)\n",
    "# g = sns.pairplot(cc_subset,hue=\"Class\",diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "66b3fcc0-dad1-4e27-ae77-bbfcec271a60",
    "_uuid": "65c77cdaac62eb70da7e9bc8d681647af464e8d0"
   },
   "source": [
    "(Plot omitted. It takes a long time and doesn't reveal much)\n",
    "\n",
    "There is not much insight form the pairplot except that most of features have clear separation for fraud records versus genuine records. We can notice that distribution of fraud records is quite different compared to genuine records in the diagonal kde plots. All the features looks to be normally distributed. So we can train the Multivariate Guassian Distribution algorthm using the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb383977-b4ce-443c-baa4-c46212acf9f9",
    "_uuid": "0261d7d87588a91343061734c69dbad45f8efa4d"
   },
   "outputs": [],
   "source": [
    "#Method for selecting epsilon with best F1-score\n",
    "def SelectThresholdByCV_Anomaly(probs,y):\n",
    "    best_epsilon = 0\n",
    "    best_f1 = 0\n",
    "    f = 0\n",
    "    precision =0\n",
    "    recall=0\n",
    "    best_recall = 0\n",
    "    best_precision = 0\n",
    "    \n",
    "    #epsilons = sorted(np.unique(probs))\n",
    "    #print(epsilons)\n",
    "    epsilons = np.arange(0,1,0.01)\n",
    "    \n",
    "    precisions=[]\n",
    "    recalls=[]\n",
    "    for epsilon in epsilons:\n",
    "        predictions = (probs < epsilon)\n",
    "        f = f1_score(y, predictions)\n",
    "        precision = precision_score(y, predictions)\n",
    "        recall = recall_score(y, predictions)\n",
    "        #print(\"Theshold {0},Precision {1},Recall {2}\".format(epsilon,precision,recall))\n",
    "          \n",
    "        if f > best_f1:\n",
    "            best_f1 = f\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "            best_epsilon = epsilon\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    #Precision-Recall Trade-off\n",
    "    plt.plot(epsilons,precisions,label='Precision')\n",
    "    plt.plot(epsilons,recalls,label='Recall')\n",
    "    plt.xlabel(\"Epsilon\")\n",
    "    plt.title('Precision Recall Trade Off')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print ('Best F1 Score %f' %best_f1)\n",
    "    print ('Associated Precision Score %f' %best_precision)\n",
    "    print ('Associated Recall Score %f' %best_recall)\n",
    "    print ('Associated Epsilon', best_epsilon)\n",
    "    return best_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "65b3dc14-2f97-4c06-b209-a59f4f94a911",
    "_uuid": "49cb3adc6274d0b04ee40bae248c16addd331021"
   },
   "outputs": [],
   "source": [
    "#Method for calculating parameters Mu & Co-variance\n",
    "def estimateGaussian(data):\n",
    "    mu = np.mean(data,axis=0)\n",
    "    sigma = np.cov(data.T)\n",
    "    return mu,sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea6dc805-9edb-4285-99b6-564483a1906a",
    "_uuid": "48f13e27237affc4a28f4df9bd46c5deb8e05090"
   },
   "outputs": [],
   "source": [
    "#Method for implementing multivariate gaussian distribution pdf, scaled\n",
    "def MultivariateGaussianDistribution(data,mu,sigma):\n",
    "    p = multivariate_normal.pdf(data, mean=mu, cov=sigma)\n",
    "    p_transformed = np.power(p,1/100) #transformed the probability scores by p^1/100 since the values are very low (up to e-150)\n",
    "    return p_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a699a497-659b-44bc-a052-97f616352b70",
    "_uuid": "a172b8651df6686679d0927cc03df7ceada19431"
   },
   "outputs": [],
   "source": [
    "genuine_data = cc_dataset[cc_dataset['Class']==0]\n",
    "fraud_data = cc_dataset[cc_dataset['Class']==1]\n",
    "\n",
    "# optionally reduce data for speed\n",
    "genuine_data = genuine_data.sample(frac=1, random_state=42)\n",
    "fraud_data = fraud_data.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "58acf65d-82ba-4bbf-8fd9-20e3f85a99dd",
    "_uuid": "ce45cbcb894e73cdea2a27f037fee7720d5f021b"
   },
   "outputs": [],
   "source": [
    "#Split Genuine records into train & test - 60:40 ratio\n",
    "genuine_train,genuine_test = train_test_split(genuine_data,test_size=0.4,random_state=0)\n",
    "print(genuine_train.shape)\n",
    "print(genuine_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "20ff9c27-4fb1-4c88-ae6d-081b521fe168",
    "_uuid": "aba4979878cdd5e83539368d57e4a2e163d3b563"
   },
   "outputs": [],
   "source": [
    "#Split 40% of Genuine Test records into Cross Validation & Test again (50:50 ratio)\n",
    "genuine_cv,genuine_test = train_test_split(genuine_test,test_size=0.5,random_state=0)\n",
    "print(genuine_cv.shape)\n",
    "print(genuine_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff9d73c8-52f1-47de-9414-75e84d75abe9",
    "_uuid": "a2adcf3d8a0dbb59aef026475be2395aeffc92f0"
   },
   "outputs": [],
   "source": [
    "#Split Fraud records into Cross Validation & Test (50:50 ratio)\n",
    "fraud_cv,fraud_test = train_test_split(fraud_data,test_size=0.5,random_state=0)\n",
    "print(fraud_cv.shape)\n",
    "print(fraud_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dcace0dc-3e71-4747-8b96-18838a81c278",
    "_uuid": "babb30b84f143fbb941435e67d4ded4efe011ff7"
   },
   "outputs": [],
   "source": [
    "#Drop Y-label from Train data\n",
    "train_data = genuine_train.drop(labels='Class',axis=1)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d0ecfd6-7770-437b-a0d0-5aab32dd81df",
    "_uuid": "352be0e31383211334638e5ca601d506db9bea58"
   },
   "outputs": [],
   "source": [
    "#Cross validation data\n",
    "cv_data = pd.concat([genuine_cv,fraud_cv])\n",
    "cv_data_y = cv_data['Class']\n",
    "cv_data.drop(labels='Class',axis=1,inplace=True)\n",
    "print(cv_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0861bef-45a6-46cc-bd5f-d34e3ddd00ff",
    "_uuid": "ad9ebd1b452f23325d87d82fec4d123a3532a110"
   },
   "outputs": [],
   "source": [
    "#Test data\n",
    "test_data = pd.concat([genuine_test,fraud_test])\n",
    "test_data_y = test_data['Class']\n",
    "test_data.drop(labels='Class',axis=1,inplace=True)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "862dc93c-79e9-4015-aba7-6f540fdfd29d",
    "_uuid": "5af4380ab204a1605d8326b79d53552639433a10"
   },
   "outputs": [],
   "source": [
    "#StandardScaler â€“ Feature scaling is not required since all the features are already standardized via PCA\n",
    "#sc = StandardScaler()\n",
    "#train_data = sc.fit_transform(train_data)\n",
    "#cv_data = sc.transform(cv_data)\n",
    "#test_data = sc.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fe37024f-c8f5-4244-bde8-ec6d6b18d49d",
    "_uuid": "25ae4e874fcb79efbe2f6ae975423c76ad670210"
   },
   "outputs": [],
   "source": [
    "#Find out the parameters Mu and Covariance for passing to the probability density function\n",
    "mu,sigma = estimateGaussian(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "baad69b5-a4db-42b5-8b5d-81ed477d9eba",
    "_uuid": "2d56e10e571774d1019d25345ba41304d801a50f"
   },
   "outputs": [],
   "source": [
    "#Multivariate Gaussian distribution - This calculates the probability for each record.\n",
    "p_train = MultivariateGaussianDistribution(train_data,mu,sigma)\n",
    "print(p_train.mean())\n",
    "print(p_train.std())\n",
    "print(p_train.max())\n",
    "print(p_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.shape, mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "779c276b-46ab-4365-882c-7dc229af5ced",
    "_uuid": "66540465744c1804c84ba9d7cff2fff9107bc16c"
   },
   "outputs": [],
   "source": [
    "#Calculate the probabilities for cross validation and test records by passing the mean and co-variance matrix derived from train data\n",
    "p_cv = MultivariateGaussianDistribution(cv_data,mu,sigma)\n",
    "p_test = MultivariateGaussianDistribution(test_data,mu,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_cv.mean())\n",
    "print(p_cv.std())\n",
    "print(p_cv.max())\n",
    "print(p_cv.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "779c276b-46ab-4365-882c-7dc229af5ced",
    "_uuid": "66540465744c1804c84ba9d7cff2fff9107bc16c"
   },
   "outputs": [],
   "source": [
    "#Calculate the probabilities for cross validation and test records by passing the mean and co-variance matrix derived from train data\n",
    "pf_cv = MultivariateGaussianDistribution(fraud_cv.drop('Class',axis=1),mu,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pf_cv.mean())\n",
    "print(pf_cv.std())\n",
    "print(pf_cv.max())\n",
    "print(pf_cv.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d8956ff8-d14a-4429-b22d-a42951085cbf",
    "_uuid": "dd340903de111fb44dd4e531d94e0fe37a98d54d"
   },
   "outputs": [],
   "source": [
    "#Let us use cross validation to find the best threshold where the F1 -score is high\n",
    "eps_optimal = SelectThresholdByCV_Anomaly(p_cv,cv_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d3ee538a-39be-4942-8484-a3a20c299988",
    "_uuid": "4c480d1df70ade498a9989490398a9e8e4a70f11"
   },
   "outputs": [],
   "source": [
    "#CV data - Predictions\n",
    "pred_cv= (p_cv < eps_optimal)\n",
    "Print_Accuracy_Scores(cv_data_y, pred_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "883f115a-af5d-491e-a8e6-567bed57e730",
    "_uuid": "9b53f13860ff7dd9fb96cca280d556dc21594d14"
   },
   "outputs": [],
   "source": [
    "#Confusion matrix on CV\n",
    "cnf_matrix = confusion_matrix(cv_data_y,pred_cv)\n",
    "row_sum = cnf_matrix.sum(axis=1,keepdims=True)\n",
    "cnf_matrix_norm =cnf_matrix / row_sum \n",
    "sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True);\n",
    "plt.title(\"Normalized Confusion Matrix - Cross Validation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f1f3fda-ba59-4ab0-97b1-5bb6cb5629a3",
    "_uuid": "6f94a8a5f78793860364d8f96c45749ed2d946bf"
   },
   "source": [
    "(These numbers in this paragraph and after will vary...)\n",
    "\n",
    "Please notice that False negatives are around 24%. I tried to reduce false negatives & improve recall score by increasing the epsilon. I was successful in bringing the recall above 80%, however precsion is going down to 70% pretty quickly. Hence I decided to choose the epsilon with best f1-score, i.e: 0.2425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22449134-3324-4e7c-9f26-1004b1ae7db6",
    "_uuid": "26e013ad809ab352fab3e5ed44882799cce85eaf"
   },
   "outputs": [],
   "source": [
    "#Test data - Check the F1-score by using the best threshold from cross validation\n",
    "pred_test = (p_test < eps_optimal)\n",
    "Print_Accuracy_Scores(test_data_y,pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dcfa8430-6e3e-4842-9c0c-642cb71efd91",
    "_uuid": "abaf7d12f66036251cd528eb84e314766871e5d6"
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(test_data_y, pred_test)\n",
    "row_sum = cnf_matrix.sum(axis=1,keepdims=True)\n",
    "cnf_matrix_norm =cnf_matrix / row_sum \n",
    "sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)\n",
    "plt.title(\"Normalized Confusion Matrix - Test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d268c06-fbb1-40e9-9cbb-3da336e971e3",
    "_uuid": "86d95da2ff2500bd45b7b71907062e29885a0f6a"
   },
   "source": [
    "Conclusion: Anomaly detection algorthm has provided decent results with F1-score of 83. We can improve recall & thus f1-score further by deriving new features based on the business knowledge. Since the features are transformed from PCA output, we couldn't understand their purpose and do feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b5be52c-3a78-41d5-b723-d21998567e64",
    "_uuid": "f5c4bdcc5090b1786a3d0d893ffccdd3527a236b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
