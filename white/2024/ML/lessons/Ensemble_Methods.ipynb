{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92389c2f-2afe-4ef6-9393-499c456f8f35",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a51f07-11af-48f0-9b52-845f61706f25",
   "metadata": {},
   "source": [
    "## Quick Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486ebd9-02a6-4282-96c9-61f3fca9bfbf",
   "metadata": {},
   "source": [
    "If one person is correct 60% of the time, how often is the majority of 99 similar people correct? (each is right 60% of the time in a binary classification task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9aa828-609f-4155-9f2c-34f496021155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial, comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2d928-f534-455a-8a76-7e3abfaccb6d",
   "metadata": {},
   "source": [
    "## Voting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bbc565-7776-4910-aa9e-1f3c10809dc8",
   "metadata": {},
   "source": [
    "Voting is a simple ensemble method. Multiple models are trained on the same data and each votes on the outcome of a classification take. The majority classification is the one chosen (this is called 'hard voting'). If the models can output a \"confidence score\" then these scores are averaged to produce a \"soft\" vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543682cf-1371-46f3-8829-89f6ed3e6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c023a07-30db-476f-af69-9c493151bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1706f93-34fb-43ae-bbe2-9542d380263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811de8d-b16a-46d0-8bc0-c1158c6df70a",
   "metadata": {},
   "source": [
    "We use the well-known 'moons' dataset with some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53deef79-f492-43b0-b2a2-3fd665e3a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0cb1c-d154-4662-8caa-fd77f4e94c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the moons dataset\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x=X[y == 0, 0], y=X[y == 0, 1], color='blue', label='Class 0', alpha=0.6, edgecolor='k')\n",
    "plt.scatter(x=X[y == 1, 0], y=X[y == 1, 1], color='red', label='Class 1', alpha=0.6, edgecolor='k')\n",
    "\n",
    "# Enhance plot aesthetics\n",
    "plt.title('Make Moons Dataset', fontsize=16)\n",
    "plt.xlabel('Feature 1', fontsize=14)\n",
    "plt.ylabel('Feature 2', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059d890-5771-4fa2-9caa-5813bbf64903",
   "metadata": {},
   "source": [
    "The `VotingClassifier` takes a list of voters as its input. It runs each submodel, trains it, then uses the voting method as the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df1fc6-6af7-4270-9757-05413449eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('logistic regression', LogisticRegression(random_state = 420)),\n",
    "        ('Decision Tree', DecisionTreeClassifier(random_state = 420)),\n",
    "        ('KNN', KNeighborsClassifier(n_neighbors=5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d4046-7b3f-4d3c-929c-f8a835ca5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b67760-00fe-4107-89e7-8f3580f6317f",
   "metadata": {},
   "source": [
    "Here's a handy way to see how each internal model does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1071f82-61f3-4818-9795-050bb43d7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(f\"{name} = {clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e10b1-cee4-4d82-94be-482ff2d7b5ac",
   "metadata": {},
   "source": [
    "And the final accuracy. In this case we see a nice improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a425ea0-3f89-477b-90f1-9e24d26e3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Voting score = {voting_clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83054879-58b0-47cb-b5e4-3ad464d31c98",
   "metadata": {},
   "source": [
    "We plot the decision boundary. Notice the class-1 boundary is pretty complicated. This can be good if we're capturing real complexity in the data. Less good if the model is being influenced unduly by a noisy training set. In general, voting methods work to reduce the variance the the resulting model, which means your 'test accuracy' score is more reliable than with a single model alone. And hopefully better, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30befc8a-793d-4e3f-b672-df411c60c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the decision boundary\n",
    "def plot_decision_boundary(clf, X, y, ax=None, cmap='coolwarm', title=\"Decision Boundary\"):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "    \n",
    "    # Predict on the mesh grid\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)\n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], color='blue', label='Class 0', edgecolor='k')\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], color='red', label='Class 1', edgecolor='k')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Plot the dataset and decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(voting_clf, X, y, title = \"Hard Voting Classifier\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f0085-3e98-46de-93f2-5cedc1d32825",
   "metadata": {},
   "source": [
    "We can implement soft voting. The internal models each support a confidence score, and the Voting Classifier will use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78eb331-db59-42c0-a853-990919042dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('logistic regression', LogisticRegression(random_state = 420)),\n",
    "        ('Decision Tree', DecisionTreeClassifier(random_state = 420)),\n",
    "        ('KNN', KNeighborsClassifier())],\n",
    "    voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b2712-918e-4c1d-9c39-ce0337fc2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_voting_clf.fit(X_train, y_train)\n",
    "print(f\"Soft voting accuracy score = {soft_voting_clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e45616-cdbf-480d-a9c4-2d04da9c4e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset and decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(soft_voting_clf, X, y, title = \"Soft Voting Classifier\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c42cf-481e-4c32-8faa-582309be543f",
   "metadata": {},
   "source": [
    "## To - Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6e052-eb93-49cd-babc-669aa5ec1069",
   "metadata": {},
   "source": [
    "Implement a voting classifier for the 'iris' dataset. Code to load the dataset is provided. Report the accuracy of your model. Use cross-validation for higher confidence in your result! Your voter can combine whicher classifiers you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3bcba-7bc9-4244-adfe-c2172fa7e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract features (X) and target labels (y)\n",
    "X = iris.data    # Feature matrix (shape: 150 x 4)\n",
    "y = iris.target  # Target vector (shape: 150,)\n",
    "\n",
    "# Optionally, access feature and target names\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "print(\"Data shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fc1bf-7765-48a7-9888-cfe25172efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Code Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa1850-1d9b-424b-a2a3-3b7ee6c5baa4",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ca370-c370-4c0f-9e6f-1cd8dbe44683",
   "metadata": {},
   "source": [
    "Bagging is a tecnique that also uses multiple models. But in this case usually the same model is trained on different random subsets of the data. When these subsets are chosen with replacement, the result is a 'bagging' model. (Without replacement results in a 'pasting' model which usually has lower accuracy, but it never hurts to check.) We only look at 'bagging' here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069cdf5-9032-45a7-a985-79f1e6f64eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910203d-fd57-4b54-a4c1-4c99ca3a1c18",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdcbd27-a017-4109-9cc2-25e2bdde2b92",
   "metadata": {},
   "source": [
    "First we try a single decision tree on the moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c5bc8-391b-416b-9f60-188b83958dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train,y_train)\n",
    "print(\"Accuracy of Decision Tree: \", tree_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59234de-bfa0-4e83-b52d-1d4bc0c0e956",
   "metadata": {},
   "source": [
    "Now we bag 100 trees, each one using a random sample of size 100 from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a950db-735c-4578-85ca-97069cca7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 100,\n",
    "                            max_samples = 100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4102ee9-45f7-4f38-a322-f4ab8e75eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "print(\"Accuracy of Bagging 500 Trees: \", bag_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cf6f7-515a-49dc-864a-451d438a4d81",
   "metadata": {},
   "source": [
    "Let's look at the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716707a-c394-43d2-b9a2-9d75c80b27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset and decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(bag_clf, X_train, y_train)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262b0a2-98ea-4027-bf21-2a5767d177ec",
   "metadata": {},
   "source": [
    "It's smoother than voting, and has a high accuracy. Bagging also tends to reduce variance when compared to a single model alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020c4f4-3ba7-4382-958b-ca8937fb3827",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab14515-3f13-44f5-a801-4498f72fe505",
   "metadata": {},
   "source": [
    "A random forest is a bagging model applied to decision trees. This is so common it exsits as a special classifier type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e4cab-63e2-4047-b602-e7696d70b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes = 16, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e27013-b969-4c81-94e2-2f89881cbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf.fit(X_train, y_train)\n",
    "print(\"Accuracy of Random Forest: \", bag_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f50b7-baf1-47e4-9252-2ba24f4906cc",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a26bbb-883c-4402-be53-2ac108b1eb73",
   "metadata": {},
   "source": [
    "It is possible by using these techniques to estimate a feature's importance. This estimate looks at the average amount of information gain in each feature, over several random decision trees. Features with the highest average information gain have the highest importance. We do a thorough treatment of feature importance in the titanic dataset.\n",
    "\n",
    "This code includes some tricks/tips you maybe haven't seen before so read it carefully! You will need to modify it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d9239-accf-4556-913d-03b12e6caa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the Titanic dataset\n",
    "data = sns.load_dataset('titanic')\n",
    "\n",
    "# Drop rows with missing target values\n",
    "data = data.dropna(subset=['survived'])\n",
    "\n",
    "# Define features and target\n",
    "X_titanic = data[['pclass', 'sex', 'age', 'fare', 'embarked', 'sibsp', 'parch']]\n",
    "y_titanic = data['survived']\n",
    "\n",
    "# Handle missing values (impute with median for numerical, most frequent for categorical)\n",
    "X_titanic.loc[:,'age'] = X_titanic['age'].fillna(X_titanic['age'].median())\n",
    "X_titanic.loc[:,'embarked'] = X_titanic['embarked'].fillna(X_titanic['embarked'].mode()[0])\n",
    "\n",
    "# Preprocess features\n",
    "numerical_features = ['age', 'fare', 'sibsp', 'parch']\n",
    "categorical_features = ['pclass', 'sex', 'embarked']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a pipeline with RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic, y_titanic, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extract the trained Random Forest model\n",
    "rf = pipeline.named_steps['classifier']\n",
    "\n",
    "# Get feature names in the order they appear in the preprocessed dataset\n",
    "feature_names = numerical_features + list(\n",
    "    pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_features)\n",
    ")\n",
    "\n",
    "# Get importances from the fitted moddel\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Sort feature importances\n",
    "# Zip feature names with their importances and sort them\n",
    "feature_importances = sorted(\n",
    "    zip(feature_names, importances), \n",
    "    key=lambda x: x[1], # this sorts by \"y\" in each (x,y) pair\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Separate the sorted names and importance values\n",
    "sorted_features, sorted_importances = zip(*feature_importances)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(sorted_importances)), sorted_importances, align='center')\n",
    "plt.xticks(range(len(sorted_features)), sorted_features, rotation=90)\n",
    "plt.title(\"Feature Importances from Titanic Dataset\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f16f2-66ea-41a8-8f34-190158320947",
   "metadata": {},
   "source": [
    "## To-Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34042f9a-89fb-477a-bd87-99f6193718ec",
   "metadata": {},
   "source": [
    "Determine the feature importance for the iris dataset. Make a similar plot to the one above (this dataset will not require all the fancy preprocessing.) We give code to load the dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973bd94-f081-4fad-b34b-da9ec72da74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract features (X) and target labels (y)\n",
    "X = iris.data    # Feature matrix (shape: 150 x 4)\n",
    "y = iris.target  # Target vector (shape: 150,)\n",
    "\n",
    "# Optionally, access feature and target names\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "print(\"Data shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63b9eb-891a-4169-90c5-99f2021d4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50527e-10a0-4a48-bce6-6a16c60ed955",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0375693-0406-4431-bde0-4ebd0cc3027f",
   "metadata": {},
   "source": [
    "Boosting is a serial technique for improving training accuracy which works by training one model repeatedly, each time focusing on the wrong predictions. The same data is re-fed into the same model, but with a weight function applied to the outcomes where misclassified observations are given higher weight in a special loss function. Repeated iterations of boosting aim to reduce this loss function by improving the underlying model. AdaBoot and Gradient Boost are common examples here. XGBoost is the famous gradient boost algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fa7e7-c2f7-4083-9e0a-d6f9429223c8",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c88ac-fc90-4eac-ac05-20a0fb0ae04e",
   "metadata": {},
   "source": [
    "Ada boost works by defining a weighted error function and increasing the weights of the missed observations. It was one of the first successful boosting techniques and is applicable to any algorithm. The learning rate controls how big the correction made is at each step. Smaller rates converge more slowly. Faster rates converge faster but are prone to over-stepping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20a41a-1516-4561-9336-291e063a3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5962fb-1f6c-40e4-a9a0-36e028645b57",
   "metadata": {},
   "source": [
    "We use more moons data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381cfca-20db-47a2-a3c3-cd4fa91ab8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons(n_samples=5000, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e213e-ce22-426d-a7ca-fa19652e5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=30, learning_rate = 0.5, random_state = 42)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e793985-43a9-4aba-b0b1-60aeda979801",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ada Boost accuracy: \", ada_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90b80e-6afb-4c51-a6d9-6433b62fd8c4",
   "metadata": {},
   "source": [
    "### To-Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa4b22-f250-48b1-9e81-535151ea6aa1",
   "metadata": {},
   "source": [
    "**TO-DO**: Make a plot of the ada-boost accuracy as a function of the learning rate. Use cross-validation on each datapoint in your graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749df19-4710-41b0-9282-a85038204c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea3c165-2af7-47f0-b7a2-4adb26c48f28",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa936b-9f79-41ea-a0c4-0655fc23ddd2",
   "metadata": {},
   "source": [
    "Gradient boost is similar to Ada Boost but focuses on minimizing the residual errors by fitting the error at each step, as opposed to Ada boost which simply tries to improve accuracy and change weights on the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526d51a-c330-444c-8299-60aa824aeb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35317b7e-4e72-4991-af23-0aaff5b1acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons(n_samples=5000, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "gb_clf = GradientBoostingClassifier(random_state = 42)\n",
    "gb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b4c53-ecee-4dcd-abc9-78bd109c3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Boost accuracy: \", gb_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0c481-6039-4d65-85a6-977418d84d77",
   "metadata": {},
   "source": [
    "### To-Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335d72a-01d1-4469-b436-bbd29ccb6829",
   "metadata": {},
   "source": [
    "**To_Do**. Make a simple Decision Tree classifier and a Gradient Boosting Classifier for titanic. Use the `X_titanic` and `y_titanic` already loaded, and reuse the `pipeline` from the Titanic code, only changing what you need for your different models. Compare the resulting accuracies of the two models. Make sure you use cross validation. Reprt your best accuracy to the class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1caf4-4b45-418e-a9e6-ce782d5d8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037e299-565a-4132-925e-bbecf3388d51",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5fb2c-2ff0-4562-a547-f0d50656153b",
   "metadata": {},
   "source": [
    "Stacking combines multiple models and makes their outputs to be input to a new model. This new model predicts the target based on the outputs of the input models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4cdd6-d353-4ec6-ac61-1d9e917557ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea46dd4-819f-4150-84d2-64e5c09f408d",
   "metadata": {},
   "source": [
    "In this example, 3 models are input to a 3-layer neural network. You specify each base model and the final model in the classifier description. Notice this model always use cross validation internally to train the final_estimator. We pass in 5 as the number of cv folds to make in each training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9a2a5-722b-4d5a-9545-d511917ac773",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators = [\n",
    "        ('logistic', LogisticRegression(random_state = 42)),\n",
    "        ('random forest', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42))\n",
    "    ],\n",
    "    final_estimator= MLPClassifier(random_state=42, hidden_layer_sizes=(40,10,5), learning_rate='adaptive', max_iter=1000),\n",
    "    cv = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b16196-c10b-4bcf-8eb1-9e1037ad0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de64bbf-ce63-479c-9c5a-0bf0ab1c6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stacked accuracy: \", stacking_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3cdebe-43be-4064-8c3b-72f93cf07509",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Individual Accuracies\\n\")\n",
    "for name, clf in stacking_clf.named_estimators_.items():\n",
    "    print(f\"{name} = {clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52776f68-8ae5-4625-8053-6bbfeba73fbc",
   "metadata": {},
   "source": [
    "### To-Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1787a-446c-4066-b1b3-1f6505efcf9b",
   "metadata": {},
   "source": [
    "Apply stacking to 3 models on the Iris dataset. Print the individual and final accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d806e6d-416c-4a63-b27c-e3d524e05f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1e405-013e-44fe-92a4-bd01bb728443",
   "metadata": {},
   "source": [
    "# Bigger To-Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9d14d-6ce4-4484-8188-3dc281c5a717",
   "metadata": {},
   "source": [
    "Load the smaller MNIST dataset we have used before (the full 60,000 dataset may take too long? I haven't tried). Make sure \"X\" is a list of 1D vectors. Using the 'feature importance' capabilities described above, determine which features (i.e. pixels) are most important. Make a 2D plot where the color of each pixel is a function of its importance. This will be a sort of heat-map of pixel importance in classifying digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688c422-f58d-47ae-b481-c12f5e8fd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
