{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d783f5-82da-4038-ad8d-0511640d0329",
   "metadata": {},
   "source": [
    "# Cancer Data Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cffd6946-2639-4aec-968a-bf3d93710881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651ccfe0-7b76-4d24-9677-cc617430f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from scipy.stats import chi2_contingency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3974a2c0-3ecb-47a7-9171-5946e32e243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./\"\n",
    "filename = \"Cancer_Data.csv\"\n",
    "filepath = os.path.join(data_root, filename)\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2980e7b7-64d9-4d2a-9023-af2d40ea6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.drop(['radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst'], axis=1, inplace=True)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bcef77-ddd8-4757-b388-d03b4a09a46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66df47d5-ec4d-4fe2-aa96-a27827b9b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cabc98d-57b1-4de6-af0a-722c5f02b998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  fractal_dimension_mean  Unnamed: 32  \n",
       "0           0.2419                 0.07871          NaN  \n",
       "1           0.1812                 0.05667          NaN  \n",
       "2           0.2069                 0.05999          NaN  \n",
       "3           0.2597                 0.09744          NaN  \n",
       "4           0.1809                 0.05883          NaN  \n",
       "..             ...                     ...          ...  \n",
       "564         0.1726                 0.05623          NaN  \n",
       "565         0.1752                 0.05533          NaN  \n",
       "566         0.1590                 0.05648          NaN  \n",
       "567         0.2397                 0.07016          NaN  \n",
       "568         0.1587                 0.05884          NaN  \n",
       "\n",
       "[569 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8a69e8-01b3-4bb6-aace-a78c0c81af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(df, one_hot_encode = False):\n",
    "   \n",
    " \n",
    "   # Use sklearn Imputers to fill in the categorical and numerical columns\n",
    "    simple_median = SimpleImputer(strategy='median')\n",
    "    simple_most_freq = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    num_cols = df.select_dtypes(include=np.number).columns # numerical data\n",
    "    cat_cols = df.select_dtypes(include=object).columns # categorical data\n",
    "\n",
    "    df[num_cols] = simple_median.fit_transform(df[num_cols])\n",
    "    df[cat_cols] = simple_most_freq.fit_transform(df[cat_cols])\n",
    "\n",
    "    if one_hot_encode:\n",
    "        O_encoder = OrdinalEncoder()\n",
    "        df[cat_cols]= O_encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "        # df = pd.get_dummies(df, dtype=int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff47867d-159d-4c30-8bd2-13646c2b83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train(df, test_size = 0.2, random_state = True):\n",
    "    target = \"diagnosis\"    \n",
    "    X = df.drop(target, axis=1)\n",
    "     # add a scaler here. It works by finding a fit first (computing mu and sigma)\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    # and then transforming the data\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    y = df[target]\n",
    "    if random_state is True:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd4b5f4-62c6-4e00-a1ad-229c91c98777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines would load the data locally\n",
    "data_root = \"./\"\n",
    "filename = \"cancer_data_cleaned.csv\"\n",
    "filepath = os.path.join(data_root, filename)\n",
    "\n",
    "# Perform a logistic regression\n",
    "df = get_data(filepath)\n",
    "df = pre_process_data(df, one_hot_encode = True)\n",
    "X_train, X_test, y_train, y_test = get_test_train(df, random_state = True)\n",
    "lreg = LogisticRegression()\n",
    "model = lreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f83cd-8046-4280-83c5-fa0cbc232b87",
   "metadata": {},
   "source": [
    "Get the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9730f9d7-6805-4f7d-b2e8-0fe4b08ed0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy  = 0.954\n",
      "Test  accuracy  = 0.912\n"
     ]
    }
   ],
   "source": [
    "pred = lreg.predict(X_test)\n",
    "print(f\"Train accuracy  = {model.score(X_train, y_train):.3}\")\n",
    "print(f\"Test  accuracy  = {model.score(X_test, y_test):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a42e13-5479-40af-ac2f-d20ada740acf",
   "metadata": {},
   "source": [
    "Quick snapshot of the confusion matrix (rows are truth  0/1 and cols are predictions 0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e090a27-f2ee-4bff-9e8b-585b596a6e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65,  2],\n",
       "       [ 8, 39]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, lreg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e10ab-5201-4902-94d8-cce010c2ff1a",
   "metadata": {},
   "source": [
    "We want to get the probabilites from X-test, NOT the classifications. So we want raw values in (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70d3d0ef-ed32-4760-ab88-2c08ad084104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the test data\n",
    "# You need to **CHANGE THIS CODE** and return only a vector of the probabilities for class = 1\n",
    "y_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78110b-0b92-4f5f-8612-20492189843c",
   "metadata": {},
   "source": [
    "If we sort y_prob and y_test in the same order, then we can make a reasonable plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3fd248e-3fd1-4444-bf99-95f5854afb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data for plotting\n",
    "sorted_indices = np.argsort(y_prob)\n",
    "sorted_y_prob = y_prob[sorted_indices]\n",
    "sorted_y_test = np.array(y_test)[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cee5f2d9-1210-41d3-b0ec-85e77c4ddceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the vector of sorted_y_prob and of sorted_y_test to verify the are generally increasing from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06778ca8-1b1e-469d-8589-96a68d373cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db40cc3a-4ccc-43a5-a88a-9ab043b64347",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3114895369.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.plot(?????????)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Plot the sigmoid curve (predicted probabilities)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the sorted_y_prob vector as a line, labeled \"Predicted probability\" in blue\n",
    "plt.plot(?????????)\n",
    "\n",
    "# Plot the sorted_y_test values as a scatter plot, labeled \"Actual Probability\" in red\n",
    "plt.scatter(?????)\n",
    "\n",
    "# Plot the cutoff line (decision boundary at 0.5)\n",
    "plt.axhline(0.5, color='red', linestyle='--', label='Decision boundary (0.5)')\n",
    "\n",
    "plt.title('Logistic Regression Sigmoid Curve')\n",
    "plt.xlabel('Feature value')\n",
    "plt.ylabel('Predicted probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed4b4d-5cfb-4619-aa77-90f8665a09c9",
   "metadata": {},
   "source": [
    "Binary classification for logistic regression relies on knowing where the 'split point' is. It default to alpha = 0.5, but this may not be optimal. You will define some helper functions to determine an optimal alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ca8c55-cb4e-476f-a050-ac7c1db6a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classify(y_prob, alpha):\n",
    "    # return a vector where index i is 1 if y_prob[i] > alpha, else 0 \n",
    "    \n",
    "def tp(true, observed): \n",
    "    # return the number of true positives, e.g. indices i where true[i] = observed[i] = 1\n",
    "\n",
    "def tn(true, observed): \n",
    "    return sum([1 if (x == 0 and y == 0) else 0 for x,y in zip(true,observed)])\n",
    "\n",
    "def fp(true, observed): \n",
    "    return sum([1 if (x == 0 and y == 1) else 0 for x,y in zip(true,observed)])\n",
    "\n",
    "def fn(true, observed): \n",
    "    return sum([1 if (x == 1 and y == 0) else 0 for x,y in zip(true,observed)])\n",
    "\n",
    "def precision(true, observed): \n",
    "    return tp(true, observed) / (tp(true, observed) + fp(true, observed))\n",
    "\n",
    "def recall(true, observed): \n",
    "    return tp(true, observed) / (tp(true, observed) + fn(true, observed))\n",
    "                                 \n",
    "def score(true, observed, weights):\n",
    "    tps, fps, tns, fns = weights\n",
    "    return tps*tp(true,observed) + tns*tn(true, observed) + fps*fp(true, observed) +fns*fn(true, observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94d304-41b0-4635-8d07-4c180f832099",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = binary_classify(sorted_y_prob, 0.5)\n",
    "q = zip(observed, sorted_y_test)\n",
    "print(precision(sorted_y_test, observed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19deaa4-b34f-4b21-b2d4-90d07377400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = []\n",
    "rs = []\n",
    "scores = []\n",
    "for i in range(100):\n",
    "    alpha = i/100.0\n",
    "    observed = binary_classify(sorted_y_prob, alpha)\n",
    "    ps += [precision(sorted_y_test, observed)]\n",
    "    rs += [recall(sorted_y_test, observed)]\n",
    "    scores += [score(sorted_y_test, observed, (0,1,0,10))]\n",
    "plt.plot(ps, rs)\n",
    "# Annotate alpha values at selected points (for example, every 10th point)\n",
    "for i in range(0, 100, 10):  # You can change the step size (10) to select more/fewer points\n",
    "    alpha = i / 100.0\n",
    "    plt.annotate(f'α={alpha:.2f}', (ps[i], rs[i]), \n",
    "                 textcoords=\"offset points\", xytext=(5, -5), ha='center')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffc3bc-c95f-4880-b99d-da409041bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,1,0.01),scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7c66d-5d09-45c6-bfde-7bc31e0c7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7e2d3-424b-428f-95a3-4bb168e4c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha = np.argmin(scores)/100\n",
    "observed = binary_classify(sorted_y_prob, opt_alpha)\n",
    "ps = precision(sorted_y_test, observed)\n",
    "rs = recall(sorted_y_test, observed)\n",
    "opt_tp = tp(sorted_y_test, observed)\n",
    "opt_tn = tn(sorted_y_test, observed)\n",
    "opt_fp = fp(sorted_y_test, observed)\n",
    "opt_fn = fn(sorted_y_test, observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9bdaec-6d2b-4c39-b877-103c56a07ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ps, rs, opt_tp, opt_tn, opt_tp, opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a7943-4a63-4f62-8407-2cf73845414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(sorted_y_test, observed, labels=lreg.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=lreg.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
