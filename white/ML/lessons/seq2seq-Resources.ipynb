{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e48134-d504-44ab-b48c-a3f5677be54a",
   "metadata": {},
   "source": [
    "# Resources for seq2seq models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9534c6-e35e-46a1-9027-5f09167ad959",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7380b9-3428-4bc2-b847-fa4b0f29df7a",
   "metadata": {},
   "source": [
    "Beam search replaces greedy search by generating a fixed width search tree over the space of all possible outputs. This version also feeds the translation into google for a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c20b8-7fc8-4f84-8f42-51de6d055c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete or comment out pip line when not needed\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "from googletrans import Translator\n",
    "\n",
    "# define your tokens\n",
    "\n",
    "START = \"starttoken\"\n",
    "END = \"endtoken\"\n",
    "\n",
    "def beam_search(a_model, sentence_en, beam_width, verbose=False):\n",
    "    X = tf.convert_to_tensor([sentence_en])  # encoder input\n",
    "    X_dec = tf.convert_to_tensor([START])  # decoder input\n",
    "    y_proba = a_model.predict((X, X_dec), verbose = 0)[0, 0]  # first token's probas\n",
    "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
    "    top_translations = [  # list of best (log_proba, translation)\n",
    "        (np.log(word_proba), layer_spanish_vectorization.get_vocabulary()[word_id])\n",
    "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
    "    ]\n",
    "    \n",
    "    # extra code – displays the top first words in verbose mode\n",
    "    if verbose:\n",
    "        print(\"Top first words:\", top_translations)\n",
    "\n",
    "    for idx in range(1, max_length):\n",
    "        candidates = []\n",
    "        for log_proba, translation in top_translations:\n",
    "            if translation.endswith(END):\n",
    "                candidates.append((log_proba, translation.replace(END, str(layer_spanish_vectorization(END)[0].numpy()))))\n",
    "                continue  # translation is finished, so don't try to extend it\n",
    "            X = tf.convert_to_tensor([sentence_en])  # encoder input\n",
    "            X_dec = tf.convert_to_tensor([START + \" \" + translation])  # decoder input\n",
    "            y_proba = a_model.predict((X, X_dec), verbose=0)[0, idx]  # last token's proba\n",
    "            for word_id, word_proba in enumerate(y_proba):\n",
    "                # word = layer_spanish_vectorization.get_vocabulary()[word_id]\n",
    "                if (word_id == 1):\n",
    "                    word_proba /= 10\n",
    "                candidates.append((log_proba + np.log(word_proba),\n",
    "                                   f\"{translation} {word_id}\"))\n",
    "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
    "        for i,(p,sentence) in enumerate(top_translations):\n",
    "            words = sentence.split(\" \")\n",
    "            words[-1] = layer_spanish_vectorization.get_vocabulary()[int(words[-1])]\n",
    "            top_translations[i] = (p, \" \".join(words))\n",
    "\n",
    "        # extra code – displays the top translation so far in verbose mode\n",
    "        if verbose:\n",
    "            print(\"\\nTop translations so far:\", top_translations)\n",
    "\n",
    "        if all([tr.endswith(END) for _, tr in top_translations]):\n",
    "            result = top_translations[0][1].replace(END, \"\").strip()\n",
    "            googled_rev = None\n",
    "            print(\"**************\", result)\n",
    "            while (not googled_rev):\n",
    "                googled_rev =  translator.translate(result, dest=\"en\")\n",
    "\n",
    "            return googled_rev.text, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfdf26-52d9-4717-99c9-9ef2659be164",
   "metadata": {},
   "source": [
    "## Attention Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd05cf4-e5e7-445f-b031-7494949a9931",
   "metadata": {},
   "source": [
    "These can be incorporated into last class's notebook for English-Spanish translation. The attention layer wraps the encoder and decoder layers and links the two by adding an attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee572989-3ff4-455a-b355-ad8f43d53c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, dropout = 0.25, return_sequences=True, return_state=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40865fef-1469-491f-aca9-da620fef13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = Lambda(concatenate_states)(encoder_state)\n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "\n",
    "attention_layer = tf.keras.layers.Attention()\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "# dropout_layer = tf.keras.layers.Dropout(0.5)\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "# Y_proba = output_layer(dropout(attention_outputs))\n",
    "Y_proba = output_layer(attention_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadf735-e8a5-46a4-ae67-3355b556f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the 'saving model' bit isn't working right now\n",
    "save_best_early_stop = SaveBestModelWithEarlyStopping(save_path=\"snapshots/translator_C.keras\", patience=2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[encoder_input_layer, decoder_input_layer],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=MaskedSparseCategoricalCrossentropy(from_logits=False), optimizer=\"nadam\",\n",
    "              metrics=[MaskedSparseCategoricalAccuracy()])\n",
    "model.fit((X_train_encoder, X_train_decoder), Y_train, epochs=20, batch_size=64,\n",
    "          validation_data=((X_valid_encoder, X_valid_decoder), Y_valid),\n",
    "           callbacks = [save_best_early_stop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
