{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSV_OlCFKOD"
      },
      "source": [
        "# Training CiFAR with Keras\n",
        "\n",
        "This simple example demonstrates how to plug TensorFlow Datasets (TFDS) into a Keras model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTBSvHcSLBzc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjI6VgOBf0v0"
      },
      "source": [
        "## Step 1: Create your input pipeline\n",
        "\n",
        "Start by building an efficient input pipeline using advices from:\n",
        "* The [Performance tips](https://www.tensorflow.org/datasets/performances) guide\n",
        "* The [Better performance with the `tf.data` API](https://www.tensorflow.org/guide/data_performance#optimize_performance) guide\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3aH3vP_XLI8"
      },
      "source": [
        "### Load a dataset\n",
        "\n",
        "Load the MNIST dataset with the following arguments:\n",
        "\n",
        "* `shuffle_files=True`: The MNIST data is only stored in a single file, but for larger datasets with multiple files on disk, it's good practice to shuffle them when training.\n",
        "* `as_supervised=True`: Returns a tuple `(img, label)` instead of a dictionary `{'image': img, 'label': label}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUMhCXhFXdHQ"
      },
      "outputs": [],
      "source": [
        "(ods_train, ods_test), ds_info = tfds.load(\n",
        "    'cifar10',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgwCFAcWXQTx"
      },
      "source": [
        "### Build a training pipeline\n",
        "\n",
        "Apply the following transformations:\n",
        "\n",
        "* `tf.data.Dataset.map`: TFDS provide images of type `tf.uint8`, while the model expects `tf.float32`. Therefore, you need to normalize images.\n",
        "* `tf.data.Dataset.cache` As you fit the dataset in memory, cache it before shuffling for a better performance.<br/>\n",
        "__Note:__ Random transformations should be applied after caching.\n",
        "* `tf.data.Dataset.shuffle`: For true randomness, set the shuffle buffer to the full dataset size.<br/>\n",
        "__Note:__ For large datasets that can't fit in memory, use `buffer_size=1000` if your system allows it.\n",
        "* `tf.data.Dataset.batch`: Batch elements of the dataset after shuffling to get unique batches at each epoch.\n",
        "* `tf.data.Dataset.prefetch`: It is good practice to end the pipeline by prefetching [for performance](https://www.tensorflow.org/guide/data_performance#prefetching)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haykx2K9XgiI"
      },
      "outputs": [],
      "source": [
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "ds_train = ods_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(512)\n",
        "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQna5jvZWOS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "RZvw1KhFWPN1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "onDcYscXWSK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbsMy4X1XVFv"
      },
      "source": [
        "### Build an evaluation pipeline\n",
        "\n",
        "Your testing pipeline is similar to the training pipeline with small differences:\n",
        "\n",
        " * You don't need to call `tf.data.Dataset.shuffle`.\n",
        " * Caching is done after batching because batches can be the same between epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0KjuDf7XiqY"
      },
      "outputs": [],
      "source": [
        "ds_test = ods_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.batch(128)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTFoji3INMEM"
      },
      "source": [
        "## Step 2: Create and train the model\n",
        "\n",
        "Plug the TFDS input pipeline into a simple Keras model, compile the model, and train it."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SFvFhqaaI9LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWqxdmS1NLKA"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='leaky_relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(128, activation='leaky_relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(128, activation='leaky_relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=30,\n",
        "    validation_data=ds_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_training_curves(history):\n",
        "  acc = history.history['sparse_categorical_accuracy']\n",
        "  val_acc = history.history['val_sparse_categorical_accuracy']\n",
        "\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs_range = range(len(acc))\n",
        "\n",
        "  plt.figure(figsize=(12, 4))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs_range, loss, label='Training Loss')\n",
        "  plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "n6dUZRHjLUjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=10,\n",
        "    validation_data=ds_test,\n",
        ")\n",
        "\n",
        "for key in new_history.history:\n",
        "    history.history[key].extend(new_history.history[key])"
      ],
      "metadata": {
        "id": "upyD_1-GN540",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_training_curves(history)"
      ],
      "metadata": {
        "id": "uU89wR4VRbrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=10,\n",
        "    validation_data=ds_test,\n",
        ")\n",
        "\n",
        "for key in new_history.history:\n",
        "    history.history[key].extend(new_history.history[key])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7UR0-pBGUKKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_training_curves(history)"
      ],
      "metadata": {
        "id": "FFtmqMMAUNeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def show_confusion_matrix(model, dataset):\n",
        "  # Get true labels from dataset\n",
        "  true_labels = []\n",
        "  for images, labels in dataset.unbatch():\n",
        "      true_labels.append(labels.numpy())\n",
        "  true_labels = np.array(true_labels)\n",
        "\n",
        "  # Predict probabilities for the test set\n",
        "  predictions = model.predict(dataset)\n",
        "\n",
        "  # Get predicted classes\n",
        "  predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "  # Generate the confusion matrix\n",
        "  cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "  # Get class names from dataset info (assuming cifar10 has 'label' feature with names)\n",
        "  class_names = ds_info.features['label'].names\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
        "  plt.xlabel('Predicted Labels')\n",
        "  plt.ylabel('True Labels')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "DJS8OvI8URIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02a0a69f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def plot_value_counts(dataset):\n",
        "  # Get true labels from dataset\n",
        "  true_labels = []\n",
        "  for images, labels in dataset.unbatch():\n",
        "      true_labels.append(labels.numpy())\n",
        "  true_labels = np.array(true_labels)\n",
        "  # Count the occurrences of each true label\n",
        "  label_counts = pd.Series(true_labels).value_counts().sort_index()\n",
        "\n",
        "  # Create a DataFrame for plotting\n",
        "  plot_df = pd.DataFrame({\n",
        "      'Label Index': label_counts.index,\n",
        "      'Count': label_counts.values,\n",
        "      'Class Name': [class_names[i] for i in label_counts.index]\n",
        "  })\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.barplot(x='Class Name', y='Count', hue='Class Name', data=plot_df, palette='viridis', legend=False)\n",
        "  plt.xlabel('Output Label')\n",
        "  plt.ylabel('Count')\n",
        "  plt.title('Counts of Each Output Label in Test Set')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_confusion_matrix(model, ds_test)"
      ],
      "metadata": {
        "id": "Yy0JC-ozViQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_value_counts(ds_test)"
      ],
      "metadata": {
        "id": "J6AHgNgzVkvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_value_counts(ds_train)"
      ],
      "metadata": {
        "id": "Ihey5gXmVnUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Improve"
      ],
      "metadata": {
        "id": "BLYfQmy2VvPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EHJ6aceWHbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w3L8rxkTWIpU"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.BatchNormalization(), # Changed to BatchNormalization\n",
        "  tf.keras.layers.Dense(1024, activation='leaky_relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(512, activation='leaky_relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(256, activation='leaky_relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# No need to call .adapt() for BatchNormalization as it learns stats per batch\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=30,\n",
        "    validation_data=ds_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_training_curves(history)"
      ],
      "metadata": {
        "id": "F9RWVEmeW15u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(1024, activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), # Added L2 regularization\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(512, activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), # Added L2 regularization\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(256, activation='leaky_relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.0001),  ## decreased learning rate\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_sparse_categorical_accuracy',  # Monitor validation accuracy\n",
        "    mode='max',             # Maximize validation accuracy\n",
        "    patience=5,          # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity.\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=30,\n",
        "    validation_data=ds_test,\n",
        "    callbacks=[early_stopping_callback] # Add early stopping callback\n",
        ")"
      ],
      "metadata": {
        "id": "S5k8-T4rXifO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_training_curves(history)"
      ],
      "metadata": {
        "id": "FAmP26jaYhpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def augment_img(image, label):\n",
        "  # Random horizontal flip\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  # Random brightness adjustment\n",
        "  image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "  # Random contrast adjustment\n",
        "  image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "  # Random crop by padding and then cropping\n",
        "  IMG_SIZE = 32\n",
        "  paddings = tf.constant([[4, 4], [4, 4], [0, 0]]) # Pad by 4 pixels on each side (total 40x40)\n",
        "  image = tf.pad(image, paddings, \"REFLECT\") # Use REFLECT padding to avoid black borders\n",
        "  image = tf.image.random_crop(image, size=[IMG_SIZE, IMG_SIZE, 3])\n",
        "  return image, label\n",
        "\n",
        "# Re-build the training pipeline with data augmentation.\n",
        "# Assumes 'ods_train', 'normalize_img', 'ds_info.splits['train'].num_examples', and 'tf.data.AUTOTUNE' are defined in prior cells.\n",
        "ds_train = ods_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "# Apply data augmentation to the training dataset after caching normalized data\n",
        "ds_train = ds_train.map(\n",
        "    augment_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(512)\n",
        "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Training dataset pipeline rebuilt with data augmentation.\")\n",
        "print(\"The new pipeline includes random horizontal flips, brightness, contrast adjustments, and random cropping.\")"
      ],
      "metadata": {
        "id": "MpMW4t2qYwWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(1024, activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), # Added L2 regularization\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(512, activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), # Added L2 regularization\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(256, activation='leaky_relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.0001),  ## decreased learning rate\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',  # Monitor validation accuracy\n",
        "    mode='max',             # Maximize validation accuracy\n",
        "    patience=5,          # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity.\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=30,\n",
        "    validation_data=ds_test,\n",
        "    callbacks=[early_stopping_callback] # Add early stopping callback\n",
        ")"
      ],
      "metadata": {
        "id": "g5GowVuLb7Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_training_curves(history)"
      ],
      "metadata": {
        "id": "IX4kVDR_b7uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "more_history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=30,\n",
        "    validation_data=ds_test,\n",
        "    callbacks=[early_stopping_callback] # Add early stopping callback\n",
        ")"
      ],
      "metadata": {
        "id": "mllZgsYDcRNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for key in more_history.history:\n",
        "    history.history[key].extend(more_history.history[key])"
      ],
      "metadata": {
        "id": "oxL9qpcWdud0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_training_curves(history)"
      ],
      "metadata": {
        "id": "UhxUY5K1eE5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jr_drZxzeGFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}