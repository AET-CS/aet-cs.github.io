{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true,
                "jupyter": {
                    "outputs_hidden": true
                }
            },
            "source": [
                "# Representing Text Features\n",
                "\n",
                "Similar to categorical features, scikit-learn offers an easy way to encode another common\n",
                "feature type, **text features**. When working with text features, it is often convenient to\n",
                "encode individual words or phrases as numerical values.\n",
                "\n",
                "Let's consider a dataset that contains a small corpus of text phrases:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-11T01:35:27.317858Z",
                    "iopub.status.busy": "2024-08-11T01:35:27.317388Z",
                    "iopub.status.idle": "2024-08-11T01:35:27.328538Z",
                    "shell.execute_reply": "2024-08-11T01:35:27.327312Z"
                }
            },
            "outputs": [],
            "source": [
                "sample = [\n",
                "    'feature engineering',\n",
                "    'feature selection',\n",
                "    'feature extraction'\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One of the simplest methods of encoding such data is by **word count**; for each phrase, we count the occurrences of each word within it. In scikit-learn, this is easily done using\n",
                "`CountVectorizer`, which functions akin to `DictVectorizer`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-11T01:35:27.333788Z",
                    "iopub.status.busy": "2024-08-11T01:35:27.333353Z",
                    "iopub.status.idle": "2024-08-11T01:35:27.853298Z",
                    "shell.execute_reply": "2024-08-11T01:35:27.852782Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
                            "\twith 6 stored elements and shape (3, 4)>"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "vec = CountVectorizer()\n",
                "X = vec.fit_transform(sample)\n",
                "X"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "By default, this will store our feature matrix `X` as a sparse matrix. If we want to manually\n",
                "inspect it, we need to convert it to a regular array:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-11T01:35:27.883775Z",
                    "iopub.status.busy": "2024-08-11T01:35:27.883459Z",
                    "iopub.status.idle": "2024-08-11T01:35:27.887680Z",
                    "shell.execute_reply": "2024-08-11T01:35:27.887327Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[1, 0, 1, 0],\n",
                            "       [0, 0, 1, 1],\n",
                            "       [0, 1, 1, 0]])"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X.toarray()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "...with corresponding feature names:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-11T01:35:27.889684Z",
                    "iopub.status.busy": "2024-08-11T01:35:27.889498Z",
                    "iopub.status.idle": "2024-08-11T01:35:27.892953Z",
                    "shell.execute_reply": "2024-08-11T01:35:27.892545Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array(['engineering', 'extraction', 'feature', 'selection'], dtype=object)"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "vec.get_feature_names_out()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One possible shortcoming of this approach is that we might put too much weight on words\n",
                "that appear very frequently. One approach to fix this is known as term **frequency-inverse\n",
                "document frequency (TF-IDF)**. What TF-IDF does might be easier to understand than its\n",
                "name, which is basically to weigh the word counts by a measure of how often they appear\n",
                "in the entire dataset.\n",
                "\n",
                "The syntax for TF-IDF is pretty much similar to the previous command:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-11T01:35:27.895083Z",
                    "iopub.status.busy": "2024-08-11T01:35:27.894908Z",
                    "iopub.status.idle": "2024-08-11T01:35:27.900198Z",
                    "shell.execute_reply": "2024-08-11T01:35:27.899825Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[0.861037  , 0.        , 0.50854232, 0.        ],\n",
                            "       [0.        , 0.        , 0.50854232, 0.861037  ],\n",
                            "       [0.        , 0.861037  , 0.50854232, 0.        ]])"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "vec = TfidfVectorizer()\n",
                "X = vec.fit_transform(sample)\n",
                "X.toarray()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We note that the numbers are now smaller than before, with the third column taking the\n",
                "biggest hit. This makes sense, as the third column corresponds to the most frequent word\n",
                "across all three phrases, `'feature'`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-11T01:35:27.902107Z",
                    "iopub.status.busy": "2024-08-11T01:35:27.901793Z",
                    "iopub.status.idle": "2024-08-11T01:35:27.905163Z",
                    "shell.execute_reply": "2024-08-11T01:35:27.904739Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array(['engineering', 'extraction', 'feature', 'selection'], dtype=object)"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "vec.get_feature_names_out()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Representing text features will become important in [Chapter 7](07.00-Implementing-a-Spam-Filter-with-Bayesian-Learning.ipynb), *Implementing a Spam Filter\n",
                "with Bayesian Learning*."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}